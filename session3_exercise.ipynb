{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb9c82a-786e-440c-8aaa-d70d1132935e",
   "metadata": {},
   "source": [
    "## Install libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dc6d3-16fe-4a6e-a2fd-516d6a2fa752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab456a8-0754-44cd-a746-877d47e8edf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc3e3f-92d2-467e-baf1-3744cc582e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4ab23-3238-4783-9bc7-37a2252db30e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587ce1d-8d13-410b-b0dd-862572758f96",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b89f8-4cbb-48ce-b859-56cf6aa0cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import google.generativeai as genai\n",
    "import vertexai\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from PyPDF2 import PdfReader\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863de07a-da94-4952-812a-3caa2ca3123b",
   "metadata": {},
   "source": [
    "### Configure Google Generative AI (Gemini) via Application Default Credentials (ADC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef9a5b-8017-4912-a530-3c608504e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses ADC automatically\n",
    "genai.configure()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca2d81-dbe4-4631-8af5-4a07574c12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK â€” ADC picks up credentials for project and auth\n",
    "vertexai.init(project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e1ae3-9851-4865-9142-1f48022e7a42",
   "metadata": {},
   "source": [
    "### Qdrant connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22775da9-6ec3-48a6-a816-5f72d9b96daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your API_key and URL for Qdrant. \n",
    "# You can get these from your Qdrant account https://login.cloud.qdrant.io/ \n",
    "API_Key = \n",
    "URL = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76f38a-1e58-4011-a5f5-72c78ee45fe3",
   "metadata": {},
   "source": [
    "### Qdrant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12aff42-2007-4efd-8b66-98377e232a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the QdrantClient\n",
    "def init_qdrant(qdrant_url: str, qdrant_api_key: str):\n",
    "    return QdrantClient(url=qdrant_url, api_key=qdrant_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8936d5-ff36-4a36-8cad-8d670523ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection to host the uploaded files in Qdrant\n",
    "def create_qdrant_collection(collection_name: str, qdrant_url: str, qdrant_api_key: str, \n",
    "                             vector_size: int = 3072, distance: str = \"Cosine\"):\n",
    "    client = init_qdrant(qdrant_url, qdrant_api_key)\n",
    "    try:\n",
    "        client.recreate_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=vector_size, distance=distance)\n",
    "        )\n",
    "    except AttributeError:\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=vector_size, distance=distance)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35395232-5d4f-4bf1-a513-a6b8b2825f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest PDFs into Qdrant \n",
    "def ingest_pdfs_to_qdrant(pdf_files, collection_name: str, qdrant_url: str, qdrant_api_key: str, chunk_size: int = 500):\n",
    "    client = init_qdrant(qdrant_url, qdrant_api_key)\n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(\"gemini-embedding-001\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        reader = PdfReader(pdf_file)\n",
    "        full_text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "        chunks = [full_text[i : i + chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "        embeddings = embedding_model.get_embeddings(chunks)\n",
    "\n",
    "        points = []\n",
    "        for emb, chunk in zip(embeddings, chunks):\n",
    "            points.append(\n",
    "                models.PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector=emb.values,\n",
    "                    payload={\"text\": chunk}\n",
    "                )\n",
    "            )\n",
    "        client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78020bd8-5681-4181-8965-1079afef1b4b",
   "metadata": {},
   "source": [
    "### Invoke LLM calls with user prompt and retrieved relevant context from the uploaded PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed9591-e89e-4899-a11d-1a48d871079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_response(messages, model_name: str, temperature: float, top_p: float, max_output: int,\n",
    "    collection_name: str, qdrant_url: str, qdrant_api_key: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    If `collection_name` is set, retrieve top-k chunks from Qdrant and prepend as context. \n",
    "    Otherwise, just chat on the conversation history.\n",
    "    \"\"\"\n",
    "    # Build conversation history to allow for multi-turn conversation\n",
    "    history = \"\"\n",
    "    for msg in messages:\n",
    "        speaker = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        history += f\"{speaker}: {msg['content']}\\n\"\n",
    "\n",
    "    # Retrieve RAG context if available\n",
    "    context = \"\"\n",
    "    if collection_name:\n",
    "        embedding_model = TextEmbeddingModel.from_pretrained(\"gemini-embedding-001\")\n",
    "        query_emb = embedding_model.get_embeddings([messages[-1][\"content\"]])[0].values\n",
    "        client = init_qdrant(qdrant_url, qdrant_api_key)\n",
    "        hits = client.search(collection_name=collection_name, query_vector=query_emb, limit=k)\n",
    "        context = \"\\n\\n\".join(hit.payload.get(\"text\", \"\") for hit in hits)\n",
    "\n",
    "    # Compose prompt\n",
    "    if context:\n",
    "        prompt = f\"Context:\\n{context}\\n\\nConversation:\\n{history}Assistant:\"\n",
    "    else:\n",
    "        prompt = f\"Conversation:\\n{history}Assistant:\"\n",
    "\n",
    "    # Call Gemini\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=model_name,\n",
    "        generation_config={\"temperature\": temperature, \"max_output_tokens\": max_output, \"top_p\": top_p}\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13790d-3ee7-4669-8f7a-d83b86c0e124",
   "metadata": {},
   "source": [
    "### Tweak and set LLM configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5652e4-d638-4409-9b3d-0bc89673b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your LLM model - Note, input a string with quotation mark -> \"model_name\"\n",
    "# Options: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite\n",
    "MODEL_NAME = \"<Enter Your Chosen Model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d084890-21a6-4e5b-a750-7051f2aa59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify temperature - Note, input a decimal number, e.g. 0.7\n",
    "TEMPERATURE = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15dc3a-425b-49c3-a2c1-843cd845a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Top_P - Note, input a decimal number, e.g. 0.9\n",
    "TOP_P = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6febb8-0f8e-49d7-9bd6-7bda95a54019",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTPUT = 8192\n",
    "DEFAULT_COLLECTION = \"pdfs_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239453fe-b7f2-4a79-a159-724c76fcbea6",
   "metadata": {},
   "source": [
    "### UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e910a7-ce93-4b0c-b236-123d421f86d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streamlit UI setup\n",
    "st.set_page_config(page_title=\"My PDF RAG Chatbot\", layout=\"wide\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"ðŸ¤– Chat with My PDFs\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"qdrant_collection\" not in st.session_state:\n",
    "    st.session_state.qdrant_collection = None\n",
    "    \n",
    "# PDF uploader and automatic ingestion (only once) - reset after url refresh\n",
    "uploaded_files = st.file_uploader(\n",
    "    \"ðŸ“„ Upload PDFs (optional â€” chat works even without PDFs)\",\n",
    "    type=[\"pdf\"],\n",
    "    accept_multiple_files=True,\n",
    ")\n",
    "if uploaded_files and not st.session_state.qdrant_collection:\n",
    "    with st.spinner(\"Creating Qdrant collection & ingesting PDFsâ€¦\"):\n",
    "        create_qdrant_collection(\n",
    "            collection_name=DEFAULT_COLLECTION,\n",
    "            qdrant_url=URL,\n",
    "            qdrant_api_key=API_Key,\n",
    "        )\n",
    "        ingest_pdfs_to_qdrant(\n",
    "            pdf_files=uploaded_files,\n",
    "            collection_name=DEFAULT_COLLECTION,\n",
    "            qdrant_url=URL,\n",
    "            qdrant_api_key=API_Key,\n",
    "        )\n",
    "        st.session_state.qdrant_collection = DEFAULT_COLLECTION\n",
    "        st.success(\"âœ… Success!! PDFs ingested! Future replies will include RAG context.\")\n",
    "        \n",
    "# Always-on chat interface: history above, input at bottom\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "if user_input := st.chat_input(\"Type your questionâ€¦\"):\n",
    "    # Append and display user turn\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    st.chat_message(\"user\").markdown(user_input)\n",
    "\n",
    "    # Generate assistant reply (with or without RAG)\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinkingâ€¦\"):\n",
    "            reply = get_bot_response(\n",
    "                messages=st.session_state.messages,\n",
    "                model_name=MODEL_NAME,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                max_output=MAX_OUTPUT,\n",
    "                collection_name=st.session_state.qdrant_collection,\n",
    "                qdrant_url=URL,\n",
    "                qdrant_api_key=API_Key,\n",
    "            )\n",
    "            st.markdown(reply)\n",
    "\n",
    "    # Save assistant turn\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
